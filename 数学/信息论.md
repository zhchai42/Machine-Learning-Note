## 熵 entropy

### 定义

对离散分布$P(X=x_i)=p_i​$，熵定义为：
$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}=H(p)
$$

> > > 又有定义：
> > > $$
> > > 0\log0=0
> > > $$
> > > 
> > >
> > > #### 条件熵
> > >
> > > 对联合概率分布$P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}$

条件熵$H(Y|X)$定义内，$X$给定条件下，$Y$的熵对$X$的期望
$$
H(Y | X)=\sum_{i=1}^{n} p_{i} H(Y | X=x_{i})
$$
其中$p_i$为$X​$的边缘概率质量。

当熵和条件熵由极大似然估计得到，分别称为经验熵 **empirical entropy** 经验条件熵 

### 互信息

定义如下：
$$
I(X;Y)=H(X)-H(H|Y)=H(Y)-H(Y|X)
$$

可以认为是在给定$Y/X$的条件下，对$X/Y$不确定性的降低。
























