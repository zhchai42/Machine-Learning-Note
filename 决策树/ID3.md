## ID3

### 定义

一种决策树的生成算法。

在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。

相当于使用极大似然法进行概率模型选择。

只使用ID3算法容易产生过拟合。

### 方法

从根结点开始，对接点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，以该特征的不同取值建立子结点

递归地使用如上方法，直到所有特征的信息增益很小或者没有特征可以选择。

#### 信息增益

特征$A$对训练数据集$D$的信息增益$g(D,A)$ 定义为训练数据集中类与特征的互信息。
$$
g(D, A)=I(D;A)=H(D)-H(D | A)
$$
可以认为是在给定特征$A$的条件下，对数据$D$分类不确定性的降低。

注意这里的熵为经验熵，一般由极大似然估计得到。

经验熵计算如下：
$$
P(C_k)=\dfrac{|C_k|}{|D|}\\
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$
经验条件熵：
$$
H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D| } \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{k}\right|}{\left|D_{i}\right|}
$$

#### 基于信息增益的特征选择

对训练数据集，计算其每个特征的信息增益，选择信息增益最大的特征。

### 算法

> 统计学习方法 P63

输入：数据集$D$，特征集$A$，阈值$\epsilon $

输出：决策树$T$

1. $D$中所有实例是否属于同一类​$C_k​$或是否还有特征可以选择
   1. 若$D$中所有实例属于同一类$C_k$ ，则$T$只有一个结点，将$C_k$作为结点的标记返回$T$      /      若$A=\varnothing$ ，则$T$只有一个结点，**将树中实例数最多的类$C_k$作为结点标记**，返回$T​$
   2. 若实例不属于同一类且还有特征可以选择，选择信息增益最大的特征$A_g$，考察$A_g​$的信息增益是否小于阈值
      1. 如果小于阈值，则$T$只有一个结点，**将树中实例数最多的类$C_k$作为结点标记**，返回$T$
      2. 否则，对$A_g$的每一个可能值$a_i$，将$D$分割为若干非空子集$D_i$，**将$D_i$中实例数最多的类$C_k$作为结点标记**，构建子结点。对第$i$个子结点，以$D_i$为训练集，$A-\{A_g\}​$为特征集，递归调用本算法。

注意最后一步中，对并非叶节点的结点（没有被确定分类的情况）同样用类别$C_k$做了标记，是由于在之后的剪枝中需要用到。

































